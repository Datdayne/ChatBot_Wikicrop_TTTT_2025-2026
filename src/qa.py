import numpy as np
import faiss
import requests
from sentence_transformers import SentenceTransformer, CrossEncoder
import os
from config_loader import load_config
import db  # Import module database m·ªõi

# --- C·∫§U H√åNH ---
config = load_config()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_FILE = os.path.join(BASE_DIR, "..", "faiss.index")

# Config path to DB is handled inside db.py via config loader, so we just use db module.

RERANK_MODEL = config["model"]["RERANK_MODEL"]
MODEL_NAME = config["model"]["embedding_model"]

# Config Performance
USE_RERANKER = config["vector_db"].get("use_reranker", True)
RETRIEVAL_TOP_K = config["vector_db"].get("retrieval_top_k", 30)
RERANK_TOP_N = config["vector_db"].get("rerank_top_n", 5)

# --- LOAD MODEL & DATA ---
print(f"‚è≥ ƒêang t·∫£i models...\n   - Embedding: {MODEL_NAME}")
embedder = SentenceTransformer(MODEL_NAME)

if USE_RERANKER:
    print(f"   - Reranker: {RERANK_MODEL}")
    reranker = CrossEncoder(RERANK_MODEL)
else:
    print("   - Reranker: OFF (Ch·∫ø ƒë·ªô Fast Mode)")
    reranker = None

# Load FAISS
if os.path.exists(INDEX_FILE):
    index = faiss.read_index(INDEX_FILE)
else:
    raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y file faiss.index! H√£y ch·∫°y ingest.py tr∆∞·ªõc.")

# Kh√¥ng load docs.json n·ªØa v√¨ ƒë√£ chuy·ªÉn sang SQLite (lazy load)

def reload_index():
    """Reload FAISS index from disk (d√πng sau khi Ingest)"""
    global index
    if os.path.exists(INDEX_FILE):
        print("üîÑ Reloading FAISS index...")
        index = faiss.read_index(INDEX_FILE)
    else:
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y index ƒë·ªÉ reload.")

# ==============================================================================
# 1. RETRIEVE & RERANK (C√ì L·ªåC NG∆Ø·ª†NG ƒêI·ªÇM)
# ==============================================================================
def retrieve(query, top_k=RETRIEVAL_TOP_K, rerank_top_n=RERANK_TOP_N, score_threshold=0.0):
    """
    T√¨m ki·∫øm v√† l·ªçc k·∫øt qu·∫£.
    - score_threshold: Ng∆∞·ª°ng ƒëi·ªÉm t·ªëi thi·ªÉu. N·∫øu ƒëi·ªÉm < 0 (ho·∫∑c th·∫•p h∆°n), b·ªè qua.
    """
    # 1. Embedding Query (Th√™m prefix query: cho E5)
    qv = embedder.encode([f"query: {query}"], normalize_embeddings=True).astype("float32")

    # 2. T√¨m ki·∫øm th√¥ b·∫±ng FAISS
    D, I = index.search(qv, top_k)
    
    # L·∫•y ra danh s√°ch ID h·ª£p l·ªá, b·ªè qua -1
    valid_ids = [int(idx) for idx in I[0] if idx != -1]
    
    # Truy v·∫•n n·ªôi dung t·ª´ SQLite theo ID
    candidates = db.get_documents_by_ids(valid_ids)

    if not candidates:
        return []

    # 3. Rerank (N·∫øu b·∫≠t)
    if USE_RERANKER:
        pairs = [(query, c["text"]) for c in candidates]
        scores = reranker.predict(pairs)
        
        # Gh√©p (candidate, score) l·∫°i v√† sort gi·∫£m d·∫ßn
        ranked_candidates = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)
    else:
        # N·∫øu t·∫Øt reranker, gi·ªØ nguy√™n th·ª© t·ª± FAISS (Kho·∫£ng c√°ch Euclid: c√†ng nh·ªè c√†ng t·ªët, nh∆∞ng FAISS inner product: c√†ng l·ªõn c√†ng t·ªët)
        # Tuy nhi√™n index ƒëang l√† inner product hay L2? Th∆∞·ªùng m·∫∑c ƒë·ªãnh l√† L2 n·∫øu kh√¥ng n√≥i g√¨. 
        # Nh∆∞ng ·ªü ƒë√¢y ta c·ª© gi·∫£ s·ª≠ FAISS tr·∫£ v·ªÅ theo th·ª© t·ª± t·ªët nh·∫•t r·ªìi.
        # G√°n score gi·∫£ ƒë·ªãnh gi·∫£m d·∫ßn ƒë·ªÉ logic b√™n d∆∞·ªõi ho·∫°t ƒë·ªông
        ranked_candidates = [(c, 1.0 - (i*0.01)) for i, c in enumerate(candidates)]

    # 4. S·∫Øp x·∫øp v√† L·ªåC (Filtering)
    results = []
    
    for i, (doc, score) in enumerate(ranked_candidates):
        # N·∫øu d√πng reranker th√¨ m·ªõi care threshold ch·∫∑t ch·∫Ω, 
        # c√≤n kh√¥ng d√πng reranker th√¨ score l√† gi·∫£ ƒë·ªãnh, n√™n b·ªè qua check threshold √¢m
        if USE_RERANKER and score < score_threshold:
            continue  # B·ªè qua k·∫øt qu·∫£ k√©m
            
        if len(results) >= rerank_top_n:
            break # ƒê√£ ƒë·ªß s·ªë l∆∞·ª£ng c·∫ßn l·∫•y

        results.append({
            "rank": len(results) + 1,
            "source": doc["source"],
            "rep_type": doc["rep_type"],
            "score": float(score),
            "text": doc["text"]
        })

    return results

# ==============================================================================
# 2. BUILD PROMPT 
# ==============================================================================
def make_prompt(query: str, retrieved: list, role: str = "Chuy√™n gia n√¥ng nghi·ªáp") -> str:
    if not retrieved:
        # N·∫øu kh√¥ng c√≥ t√†i li·ªáu n√†o v∆∞·ª£t qua ng∆∞·ª°ng ƒëi·ªÉm
        return None

    # Gh√©p ng·ªØ c·∫£nh
    parts = []
    for i, r in enumerate(retrieved, 1):
        # L√†m s·∫°ch text m·ªôt ch√∫t
        clean_text = r["text"].replace("\n", " ").strip()
        parts.append(f"T√†i li·ªáu [{i}] (Ngu·ªìn: {r['source']}):\n{clean_text}")

    context = "\n\n".join(parts)

    # Prompt Engineering: "Guardrails" (H√†ng r√†o b·∫£o v·ªá)
    prompt = (
        f"B·∫°n l√† {role}. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.\n"
        f"---------------------\n"
        f"{context}\n"
        f"---------------------\n"
        f"C√¢u h·ªèi: {query}\n\n"
        f"Y√™u c·∫ßu:\n"
        f"1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong c√°c t√†i li·ªáu tr√™n ƒë·ªÉ tr·∫£ l·ªùi.\n"
        f"2. N·∫øu t√†i li·ªáu kh√¥ng ch·ª©a c√¢u tr·∫£ l·ªùi, h√£y n√≥i: 'Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu'.\n"
        f"3. Kh√¥ng t·ª± b·ªãa ƒë·∫∑t th√¥ng tin ho·∫∑c d√πng ki·∫øn th·ª©c b√™n ngo√†i.\n"
        f"4. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch v√† tr√≠ch d·∫´n ngu·ªìn (V√≠ d·ª•: [T√†i li·ªáu 1]).\n"
        f"C√¢u tr·∫£ l·ªùi:"
    )
    return prompt

# ==============================================================================
# 3. CALL OLLAMA 
# ==============================================================================
def call_ollama(prompt: str, model: str = "qwen2.5", temperature: float = 0.3) -> str:
    """
    G·ªçi Ollama. M·∫∑c ƒë·ªãnh d√πng qwen2.5 (n·∫øu m√°y y·∫øu d√πng qwen2.5:3b)
    Temperature th·∫•p (0.3) ƒë·ªÉ model b·ªõt "s√°ng t·∫°o" lung tung.
    """
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperature, 
            "num_predict": 1024
        }
    }

    try:
        resp = requests.post(url, json=payload, timeout=60)
        resp.raise_for_status()
        return resp.json().get("response", "L·ªói: Model kh√¥ng ph·∫£n h·ªìi.")
    except Exception as e:
        return f"L·ªói k·∫øt n·ªëi Ollama: {e}"

# ==============================================================================
# 4. MAIN FLOW
# ==============================================================================
def answer(query: str, model: str = "qwen2.5", debug: bool = True) -> str:
    try:
        retrieved = retrieve(query)

        if debug:
            print(f"\n=== üîç Debug: T√¨m th·∫•y {len(retrieved)} t√†i li·ªáu ph√π h·ª£p ===")
            for r in retrieved:
                print(f"[{r['score']:.4f}] {r['source']} ({r['rep_type']})")
            print("===================================================\n")

        # 2. T·∫°o Prompt
        prompt = make_prompt(query, retrieved)
        
        # N·∫øu kh√¥ng c√≥ t√†i li·ªáu n√†o qua ƒë∆∞·ª£c v√≤ng g·ª≠i xe
        if prompt is None:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu c·ªßa b·∫°n (ƒêi·ªÉm tin c·∫≠y qu√° th·∫•p)."

        # 3. G·ªçi LLM
        return call_ollama(prompt, model=model)

    except Exception as e:
        return f"L·ªói h·ªá th·ªëng: {str(e)}"