import numpy as np
import faiss
import json
import requests
from sentence_transformers import SentenceTransformer, CrossEncoder
import os
from config_loader import load_config

# --- C·∫§U H√åNH ---
config = load_config()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_FILE = os.path.join(BASE_DIR, "..", "faiss.index")
META_FILE = os.path.join(BASE_DIR, "..", "docs.json")

RERANK_MODEL = config["model"]["RERANK_MODEL"]
MODEL_NAME = config["model"]["embedding_model"]

# --- LOAD MODEL & DATA ---
print(f"‚è≥ ƒêang t·∫£i models...\n   - Embedding: {MODEL_NAME}\n   - Reranker: {RERANK_MODEL}")
embedder = SentenceTransformer(MODEL_NAME)
reranker = CrossEncoder(RERANK_MODEL)

# Load FAISS
if os.path.exists(INDEX_FILE):
    index = faiss.read_index(INDEX_FILE)
else:
    raise FileNotFoundError("‚ùå Kh√¥ng t√¨m th·∫•y file faiss.index! H√£y ch·∫°y ingest.py tr∆∞·ªõc.")

# Load Metadata
if os.path.exists(META_FILE):
    with open(META_FILE, "r", encoding="utf-8") as f:
        docs = json.load(f)
else:
    docs = []

# ==============================================================================
# 1. RETRIEVE & RERANK (C√ì L·ªåC NG∆Ø·ª†NG ƒêI·ªÇM)
# ==============================================================================
def retrieve(query, top_k=30, rerank_top_n=5, score_threshold=0.0):
    """
    T√¨m ki·∫øm v√† l·ªçc k·∫øt qu·∫£.
    - score_threshold: Ng∆∞·ª°ng ƒëi·ªÉm t·ªëi thi·ªÉu. N·∫øu ƒëi·ªÉm < 0 (ho·∫∑c th·∫•p h∆°n), b·ªè qua.
    """
    # 1. Embedding Query
    qv = embedder.encode([query], normalize_embeddings=True).astype("float32")

    # 2. T√¨m ki·∫øm th√¥ b·∫±ng FAISS
    D, I = index.search(qv, top_k)
    
    candidates = []
    # L·∫•y ra danh s√°ch candidate, b·ªè qua -1 (kh√¥ng t√¨m th·∫•y)
    for idx in I[0]:
        if idx != -1 and idx < len(docs):
            candidates.append(docs[idx])

    if not candidates:
        return []

    # 3. Rerank b·∫±ng CrossEncoder (Ch√≠nh x√°c h∆°n Cosine)
    pairs = [(query, c["text"]) for c in candidates]
    scores = reranker.predict(pairs)

    # 4. S·∫Øp x·∫øp v√† L·ªåC (Filtering)
    results = []
    # Gh√©p (candidate, score) l·∫°i v√† sort gi·∫£m d·∫ßn
    ranked_candidates = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)

    for i, (doc, score) in enumerate(ranked_candidates):
        if score < score_threshold:
            continue  # B·ªè qua k·∫øt qu·∫£ k√©m
            
        if len(results) >= rerank_top_n:
            break # ƒê√£ ƒë·ªß s·ªë l∆∞·ª£ng c·∫ßn l·∫•y

        results.append({
            "rank": len(results) + 1,
            "source": doc["source"],
            "rep_type": doc["rep_type"],
            "score": float(score),
            "text": doc["text"]
        })

    return results

# ==============================================================================
# 2. BUILD PROMPT 
# ==============================================================================
def make_prompt(query: str, retrieved: list, role: str = "Chuy√™n gia n√¥ng nghi·ªáp") -> str:
    if not retrieved:
        # N·∫øu kh√¥ng c√≥ t√†i li·ªáu n√†o v∆∞·ª£t qua ng∆∞·ª°ng ƒëi·ªÉm
        return None

    # Gh√©p ng·ªØ c·∫£nh
    parts = []
    for i, r in enumerate(retrieved, 1):
        # L√†m s·∫°ch text m·ªôt ch√∫t
        clean_text = r["text"].replace("\n", " ").strip()
        parts.append(f"T√†i li·ªáu [{i}] (Ngu·ªìn: {r['source']}):\n{clean_text}")

    context = "\n\n".join(parts)

    # Prompt Engineering: "Guardrails" (H√†ng r√†o b·∫£o v·ªá)
    prompt = (
        f"B·∫°n l√† {role}. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.\n"
        f"---------------------\n"
        f"{context}\n"
        f"---------------------\n"
        f"C√¢u h·ªèi: {query}\n\n"
        f"Y√™u c·∫ßu:\n"
        f"1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin trong c√°c t√†i li·ªáu tr√™n ƒë·ªÉ tr·∫£ l·ªùi.\n"
        f"2. N·∫øu t√†i li·ªáu kh√¥ng ch·ª©a c√¢u tr·∫£ l·ªùi, h√£y n√≥i: 'Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu'.\n"
        f"3. Kh√¥ng t·ª± b·ªãa ƒë·∫∑t th√¥ng tin ho·∫∑c d√πng ki·∫øn th·ª©c b√™n ngo√†i.\n"
        f"4. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, s√∫c t√≠ch v√† tr√≠ch d·∫´n ngu·ªìn (V√≠ d·ª•: [T√†i li·ªáu 1]).\n"
        f"C√¢u tr·∫£ l·ªùi:"
    )
    return prompt

# ==============================================================================
# 3. CALL OLLAMA 
# ==============================================================================
def call_ollama(prompt: str, model: str = "qwen2.5", temperature: float = 0.3) -> str:
    """
    G·ªçi Ollama. M·∫∑c ƒë·ªãnh d√πng qwen2.5 (n·∫øu m√°y y·∫øu d√πng qwen2.5:3b)
    Temperature th·∫•p (0.3) ƒë·ªÉ model b·ªõt "s√°ng t·∫°o" lung tung.
    """
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperature, 
            "num_predict": 1024
        }
    }

    try:
        resp = requests.post(url, json=payload, timeout=60)
        resp.raise_for_status()
        return resp.json().get("response", "L·ªói: Model kh√¥ng ph·∫£n h·ªìi.")
    except Exception as e:
        return f"L·ªói k·∫øt n·ªëi Ollama: {e}"

# ==============================================================================
# 4. MAIN FLOW
# ==============================================================================
def answer(query: str, model: str = "qwen2.5", debug: bool = True) -> str:
    try:
        retrieved = retrieve(query, top_k=30, rerank_top_n=5, score_threshold=0.0)

        if debug:
            print(f"\n=== üîç Debug: T√¨m th·∫•y {len(retrieved)} t√†i li·ªáu ph√π h·ª£p ===")
            for r in retrieved:
                print(f"[{r['score']:.4f}] {r['source']} ({r['rep_type']})")
            print("===================================================\n")

        # 2. T·∫°o Prompt
        prompt = make_prompt(query, retrieved)
        
        # N·∫øu kh√¥ng c√≥ t√†i li·ªáu n√†o qua ƒë∆∞·ª£c v√≤ng g·ª≠i xe
        if prompt is None:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu c·ªßa b·∫°n (ƒêi·ªÉm tin c·∫≠y qu√° th·∫•p)."

        # 3. G·ªçi LLM
        return call_ollama(prompt, model=model)

    except Exception as e:
        return f"L·ªói h·ªá th·ªëng: {str(e)}"