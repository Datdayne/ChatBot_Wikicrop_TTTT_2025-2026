import os
import json
import uuid
import numpy as np
import faiss
import requests
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from extractors import auto_extract
from utils import extract_summary, extract_keywords
from config_loader import load_config

# --- CONFIG ---
config = load_config()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_FILE = os.path.join(BASE_DIR, "..", "faiss.index")
META_FILE = os.path.join(BASE_DIR, "..", "docs.json")
DATA_DIR = os.path.join(BASE_DIR, "..", "data_output")

# Cáº¥u hÃ¬nh API Wiki
WIKI_API_URL = "http://localhost/wikicrop/api.php"

MODEL_NAME = config["model"]["embedding_model"]
print(f"ğŸ”„ Äang táº£i model: {MODEL_NAME}...")
embedder = SentenceTransformer(MODEL_NAME)
dimension = embedder.get_sentence_embedding_dimension()

# --- CHUNKING ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# --- LOAD FAISS ---
if os.path.exists(INDEX_FILE) and os.path.exists(META_FILE):
    print("ğŸ“‚ Táº£i index cÅ©...")
    index = faiss.read_index(INDEX_FILE)
    with open(META_FILE, "r", encoding="utf-8") as f:
        docs = json.load(f)
else:
    print("âœ¨ Táº¡o index má»›i...")
    index = faiss.IndexFlatIP(dimension)
    docs = []

# Táº­p há»£p cÃ¡c nguá»“n Ä‘Ã£ xá»­ lÃ½ Ä‘á»ƒ trÃ¡nh trÃ¹ng
processed_sources = set(d['full_path'] for d in docs)

# ==============================================================================
# PHáº¦N 1: Xá»¬ LÃ Ná»˜I DUNG (Chunk -> Embed)
# ==============================================================================
# Sá»­a dÃ²ng Ä‘á»‹nh nghÄ©a hÃ m: thÃªm force_update=False
def process_content(text, source_name, full_identifier, source_type="file", force_update=False):
    """HÃ m chung Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n -> Chunk -> Embed"""
    
    # Logic kiá»ƒm tra trÃ¹ng láº·p:
    # Náº¿u KHÃ”NG PHáº¢I lÃ  Ã©p buá»™c (force=False) VÃ€ Ä‘Ã£ tá»“n táº¡i -> ThÃ¬ má»›i bá» qua
    if not force_update:
        if source_type == "file" and full_identifier in processed_sources:
            return [], []
        # Vá»›i Wiki, náº¿u khÃ´ng force thÃ¬ cÅ©ng bá» qua náº¿u Ä‘Ã£ cÃ³
        if source_type == "wiki" and full_identifier in processed_sources:
             return [], []

    if not text or not text.strip():
        return [], []

    # ... (Pháº§n chunking vÃ  embedding bÃªn dÆ°á»›i giá»¯ nguyÃªn) ...

    # 1. Chunking
    if len(text) > 1200:
        chunks = text_splitter.split_text(text)
    else:
        chunks = [text]

    vecs = []
    metas = []

    # 2. Embedding tá»«ng chunk
    for i, chunk_text in enumerate(chunks):
        doc_id = str(uuid.uuid4())
        
        display_source = source_name
        if len(chunks) > 1:
            display_source += f" (Äoáº¡n {i+1})"

        # Embed ná»™i dung chunk (DÃ¹ng chÃ­nh chunk_text lÃ m input cho chÃ­nh xÃ¡c)
        # Náº¿u muá»‘n dÃ¹ng summary, cÃ³ thá»ƒ bá» comment cÃ¡c dÃ²ng dÆ°á»›i
        # summary_text = extract_summary(chunk_text)
        embed_input = chunk_text 

        vec = embedder.encode(embed_input, normalize_embeddings=True)

        meta = {
            "id": doc_id,
            "source": display_source,
            "rep_type": "wiki_content" if source_type == "wiki" else "file_content",
            "text": chunk_text,
            "full_path": full_identifier
        }
        
        vecs.append(vec)
        metas.append(meta)

    return vecs, metas

# ==============================================================================
# PHáº¦N 2: HÃšT Dá»® LIá»†U Tá»ª MEDIAWIKI API (ÄÃ£ nÃ¢ng cáº¥p Pagination)
# ==============================================================================
def fetch_all_wiki_pages():
    """Láº¥y TOÃ€N Bá»˜ bÃ i viáº¿t tá»« Wiki (Xá»­ lÃ½ phÃ¢n trang chuáº©n + Láº¥y raw wikitext)"""
    print(f"ğŸŒ Äang káº¿t ná»‘i tá»›i Wiki: {WIKI_API_URL}")
    
    session = requests.Session()
    
    # Tham sá»‘ cÆ¡ báº£n (ChÆ°a cÃ³ token phÃ¢n trang)
    base_params = {
        "action": "query",
        "generator": "allpages",
        "gaplimit": "max",     # Láº¥y tá»‘i Ä‘a sá»‘ lÆ°á»£ng má»—i láº§n gá»i
        "prop": "revisions",   # Láº¥y phiÃªn báº£n sá»­a Ä‘á»•i (raw content)
        "rvprop": "content",   # Ná»™i dung
        "rvslots": "main",     # Slot chÃ­nh
        "format": "json"
    }

    results = []
    last_continue = {} # Biáº¿n lÆ°u dáº¥u váº¿t Ä‘á»ƒ láº­t trang
    page_count = 0

    # --- VÃ’NG Láº¶P VÃ‰T Cáº N (Pagination Loop) ---
    while True:
        # Trá»™n tham sá»‘ cÆ¡ báº£n vá»›i token tiáº¿p theo (náº¿u cÃ³)
        params = {**base_params, **last_continue}
        
        try:
            resp = session.get(WIKI_API_URL, params=params)
            data = resp.json()
            
            # Xá»­ lÃ½ lá»—i API náº¿u cÃ³
            if "error" in data:
                print(f"âŒ API Error: {data['error']}")
                break

            # 1. Xá»­ lÃ½ dá»¯ liá»‡u Ä‘á»£t nÃ y
            pages = data.get("query", {}).get("pages", {})
            
            for page_id, page_data in pages.items():
                title = page_data.get("title", "")
                
                # Bá» qua cÃ¡c trang há»‡ thá»‘ng (Namespace != 0)
                ns = page_data.get("ns", 0)
                if ns != 0: continue

                # Láº¥y ná»™i dung thÃ´ (Wikitext) tá»« cáº¥u trÃºc JSON
                content = ""
                try:
                    revisions = page_data.get("revisions", [])
                    if revisions:
                        content = revisions[0]["slots"]["main"]["*"]
                except KeyError:
                    pass

                if content:
                    fake_url = f"wiki://{title}"
                    results.append((title, content, fake_url))
                    page_count += 1
            
            print(f"   ... ÄÃ£ quÃ©t Ä‘Æ°á»£c {page_count} bÃ i viáº¿t...")

            # 2. Kiá»ƒm tra xem cÃ²n trang sau khÃ´ng? (Quan trá»ng)
            if 'continue' in data:
                last_continue = data['continue'] # Láº¥y token Ä‘á»ƒ Ä‘i tiáº¿p vÃ²ng sau
            else:
                break # Háº¿t dá»¯ liá»‡u rá»“i, thoÃ¡t vÃ²ng láº·p

        except Exception as e:
            print(f"âŒ Lá»—i khi quÃ©t Wiki: {e}")
            break
            
    print(f"âœ… ÄÃ£ táº£i xong Táº¤T Cáº¢ {len(results)} bÃ i viáº¿t tá»« Wiki.")
    return results

def ingest_wiki():
    print("\n--- ğŸŒ Báº®T Äáº¦U QUÃ‰T WIKI ONLINE ---")
    pages = fetch_all_wiki_pages()
    
    if not pages:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trÃªn Wiki hoáº·c lá»—i káº¿t ná»‘i.")
        return
    
    new_vectors = []
    new_metas = []

    for title, content, url in tqdm(pages, desc="Processing Wiki", unit="page"):
        v, m = process_content(content, f"Wiki: {title}", url, source_type="wiki")
        if v:
            new_vectors.extend(v)
            new_metas.extend(m)

    if new_vectors:
        _save_batch(new_vectors, new_metas)
        print(f"ğŸ‰ ÄÃ£ thÃªm {len(new_metas)} Ä‘oáº¡n vÄƒn tá»« Wiki vÃ o bá»™ nhá»›.")
    else:
        print("â© KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i tá»« Wiki Ä‘á»ƒ cáº­p nháº­t.")

# ==============================================================================
# PHáº¦N 3: HÃšT Dá»® LIá»†U Tá»ª FILE LOCAL
# ==============================================================================
def ingest_local_files(root_folder=DATA_DIR):
    print(f"\n--- ğŸ“‚ Báº®T Äáº¦U QUÃ‰T FILE LOCAL ({root_folder}) ---")
    docx_files = []
    for dirpath, _, filenames in os.walk(root_folder):
        for f in filenames:
            if f.lower().endswith(".docx") and not f.startswith("~$"):
                docx_files.append(os.path.join(dirpath, f))

    new_vectors = []
    new_metas = []
    
    for path in tqdm(docx_files, desc="Processing Files", unit="file"):
        try:
            raw_text = auto_extract(path)
            v, m = process_content(raw_text, os.path.basename(path), path, source_type="file")
            if v:
                new_vectors.extend(v)
                new_metas.extend(m)
        except Exception as e:
            print(f"Lá»—i file {path}: {e}")

    if new_vectors:
        _save_batch(new_vectors, new_metas)
        print(f"ğŸ‰ ÄÃ£ thÃªm {len(new_metas)} Ä‘oáº¡n vÄƒn tá»« File vÃ o bá»™ nhá»›.")

# --- Helper lÆ°u Ä‘Ä©a ---
def _save_batch(vectors, metas):
    if not vectors: return
    vecs_np = np.vstack(vectors).astype("float32")
    index.add(vecs_np)
    docs.extend(metas)
    faiss.write_index(index, INDEX_FILE)
    with open(META_FILE, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)
    
    for m in metas:
        processed_sources.add(m['full_path'])

# ==============================================================================
# MAIN
# ==============================================================================
if __name__ == "__main__":
    # LÆ°u Ã½: NÃªn xÃ³a file faiss.index vÃ  docs.json trÆ°á»›c khi cháº¡y náº¿u muá»‘n lÃ m má»›i hoÃ n toÃ n
    
    # 1. QuÃ©t file docx
    ingest_local_files()
    
    # 2. QuÃ©t bÃ i viáº¿t trÃªn Wiki
    ingest_wiki()
    
    print("\nâœ… HOÃ€N Táº¤T TOÃ€N Bá»˜ QUÃ TRÃŒNH Há»ŒC Dá»® LIá»†U!")