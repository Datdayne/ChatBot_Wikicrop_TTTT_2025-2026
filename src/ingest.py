import os
import uuid
import numpy as np
import faiss
import requests
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from extractors import auto_extract
from config_loader import load_config
import db  # Import module database má»›i

# --- CONFIG ---
config = load_config()

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INDEX_FILE = os.path.join(BASE_DIR, "..", "faiss.index")
DATA_DIR = os.path.join(BASE_DIR, "..", "data_output")

# Cáº¥u hÃ¬nh API Wiki
WIKI_API_URL = "http://localhost/wikicrop/api.php"

MODEL_NAME = config["model"]["embedding_model"]
print(f"ğŸ”„ Äang táº£i model: {MODEL_NAME}...")
embedder = SentenceTransformer(MODEL_NAME)
dimension = embedder.get_sentence_embedding_dimension()

# --- CHUNKING ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)

# --- LOAD FAISS ---
if os.path.exists(INDEX_FILE):
    print("ğŸ“‚ Táº£i index cÅ©...")
    index = faiss.read_index(INDEX_FILE)
else:
    print("âœ¨ Táº¡o index má»›i...")
    # DÃ¹ng IndexIDMap Ä‘á»ƒ quáº£n lÃ½ ID thá»§ cÃ´ng (khá»›p vÃ³i DB)
    index = faiss.IndexIDMap(faiss.IndexFlatIP(dimension))

# Láº¥y danh sÃ¡ch nguá»“n Ä‘Ã£ xá»­ lÃ½ tá»« DB
processed_sources = db.get_all_full_paths()

def remove_from_processed(full_path):
    if full_path in processed_sources:
        processed_sources.remove(full_path)

# ==============================================================================
# PHáº¦N 1: Xá»¬ LÃ Ná»˜I DUNG (Chunk -> Embed)
# ==============================================================================
def process_content(text, source_name, full_identifier, source_type="file", force_update=False):
    """HÃ m chung Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n -> Chunk -> Embed"""
    
    if not force_update:
        if full_identifier in processed_sources:
            return [], []

    if not text or not text.strip():
        return [], []

    # 1. Chunking
    if len(text) > 1200:
        chunks = text_splitter.split_text(text)
    else:
        chunks = [text]

    vecs = []
    # Thay vÃ¬ lÆ°u meta dict hoÃ n chá»‰nh, ta lÆ°u dá»¯ liá»‡u raw Ä‘á»ƒ insert DB
    db_entries = []

    # 2. Embedding tá»«ng chunk
    for i, chunk_text in enumerate(chunks):
        doc_uuid = str(uuid.uuid4())
        
        display_source = source_name
        if len(chunks) > 1:
            display_source += f" (Äoáº¡n {i+1})"

        embed_input = f"passage: {chunk_text}" 
        vec = embedder.encode(embed_input, normalize_embeddings=True)

        entry = {
            "doc_uuid": doc_uuid,
            "source": display_source,
            "rep_type": "wiki_content" if source_type == "wiki" else "file_content",
            "text": chunk_text,
            "full_path": full_identifier
        }
        
        vecs.append(vec)
        db_entries.append(entry)

    return vecs, db_entries

# ==============================================================================
# PHáº¦N 2: HÃšT Dá»® LIá»†U Tá»ª MEDIAWIKI API
# ==============================================================================
def fetch_all_wiki_pages():
    """Láº¥y TOÃ€N Bá»˜ bÃ i viáº¿t tá»« Wiki"""
    print(f"ğŸŒ Äang káº¿t ná»‘i tá»›i Wiki: {WIKI_API_URL}")
    
    session = requests.Session()
    base_params = {
        "action": "query",
        "generator": "allpages",
        "gaplimit": "max",
        "prop": "revisions",
        "rvprop": "content",
        "rvslots": "main",
        "format": "json"
    }

    results = []
    last_continue = {}
    page_count = 0

    while True:
        params = {**base_params, **last_continue}
        try:
            resp = session.get(WIKI_API_URL, params=params)
            data = resp.json()
            
            if "error" in data:
                print(f"âŒ API Error: {data['error']}")
                break

            pages = data.get("query", {}).get("pages", {})
            for page_id, page_data in pages.items():
                title = page_data.get("title", "")
                ns = page_data.get("ns", 0)
                if ns != 0: continue

                content = ""
                try:
                    revisions = page_data.get("revisions", [])
                    if revisions:
                        content = revisions[0]["slots"]["main"]["*"]
                except KeyError:
                    pass

                if content:
                    fake_url = f"wiki://{title}"
                    results.append((title, content, fake_url))
                    page_count += 1
            
            print(f"   ... ÄÃ£ quÃ©t Ä‘Æ°á»£c {page_count} bÃ i viáº¿t...")

            if 'continue' in data:
                last_continue = data['continue']
            else:
                break

        except Exception as e:
            print(f"âŒ Lá»—i khi quÃ©t Wiki: {e}")
            break
            
    print(f"âœ… ÄÃ£ táº£i xong Táº¤T Cáº¢ {len(results)} bÃ i viáº¿t tá»« Wiki.")
    return results

def ingest_wiki():
    print("\n--- ğŸŒ Báº®T Äáº¦U QUÃ‰T WIKI ONLINE ---")
    pages = fetch_all_wiki_pages()
    
    if not pages:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o trÃªn Wiki hoáº·c lá»—i káº¿t ná»‘i.")
        return
    
    new_vectors = []
    new_db_entries = []

    for title, content, url in tqdm(pages, desc="Processing Wiki", unit="page"):
        v, m = process_content(content, f"Wiki: {title}", url, source_type="wiki")
        if v:
            new_vectors.extend(v)
            new_db_entries.extend(m)

    if new_vectors:
        save_batch(new_vectors, new_db_entries)
        print(f"ğŸ‰ ÄÃ£ thÃªm {len(new_db_entries)} Ä‘oáº¡n vÄƒn tá»« Wiki vÃ o bá»™ nhá»›.")
    else:
        print("â© KhÃ´ng cÃ³ dá»¯ liá»‡u má»›i tá»« Wiki Ä‘á»ƒ cáº­p nháº­t.")

# ==============================================================================
# PHáº¦N 3: HÃšT Dá»® LIá»†U Tá»ª FILE LOCAL
# ==============================================================================
def ingest_local_files(root_folder=DATA_DIR):
    print(f"\n--- ğŸ“‚ Báº®T Äáº¦U QUÃ‰T FILE LOCAL ({root_folder}) ---")
    docx_files = []
    for dirpath, _, filenames in os.walk(root_folder):
        for f in filenames:
            if f.lower().endswith(".docx") and not f.startswith("~$"):
                docx_files.append(os.path.join(dirpath, f))

    new_vectors = []
    new_db_entries = []
    
    for path in tqdm(docx_files, desc="Processing Files", unit="file"):
        try:
            raw_text = auto_extract(path)
            v, m = process_content(raw_text, os.path.basename(path), path, source_type="file")
            if v:
                new_vectors.extend(v)
                new_db_entries.extend(m)
        except Exception as e:
            print(f"Lá»—i file {path}: {e}")

    if new_vectors:
        save_batch(new_vectors, new_db_entries)
        print(f"ğŸ‰ ÄÃ£ thÃªm {len(new_db_entries)} Ä‘oáº¡n vÄƒn tá»« File vÃ o bá»™ nhá»›.")

# --- Helper lÆ°u Ä‘Ä©a ---
def save_batch(vectors, db_entries):
    """
    vectors: list of numpy arrays
    db_entries: list of dicts (chÆ°a cÃ³ ID)
    """
    if not vectors: return
    
    # Láº¥y ID báº¯t Ä‘áº§u hiá»‡n táº¡i tá»« DB (Ä‘á»ƒ khá»›p vá»›i FAISS index)
    start_id = db.get_doc_count()
    
    # GÃ¡n ID cho cÃ¡c entry má»›i
    final_db_entries = []
    for i, entry in enumerate(db_entries):
        entry['id'] = start_id + i
        final_db_entries.append(entry)
        # update tracking set
        processed_sources.add(entry['full_path'])

    # 1. ThÃªm vÃ o FAISS (vá»›i ID cá»¥ thá»ƒ)
    vecs_np = np.vstack(vectors).astype("float32")
    ids_np = np.array([e['id'] for e in final_db_entries], dtype=np.int64)
    index.add_with_ids(vecs_np, ids_np)
    
    faiss.write_index(index, INDEX_FILE)

    # 2. ThÃªm vÃ o SQLite
    db.add_documents_batch(final_db_entries)

# ==============================================================================
# MAIN
# ==============================================================================
if __name__ == "__main__":
    # Äáº£m báº£o DB Ä‘Æ°á»£c khá»Ÿi táº¡o
    db.init_db()
    
    # 1. QuÃ©t file docx
    ingest_local_files()
    
    # 2. QuÃ©t bÃ i viáº¿t trÃªn Wiki
    ingest_wiki()
    
    print("\nâœ… HOÃ€N Táº¤T TOÃ€N Bá»˜ QUÃ TRÃŒNH Há»ŒC Dá»® LIá»†U!")